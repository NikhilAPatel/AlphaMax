{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-25T17:45:02.759714Z",
     "end_time": "2023-04-25T17:45:02.761129Z"
    }
   },
   "outputs": [],
   "source": [
    "import optuna as optuna\n",
    "from stable_baselines3 import PPO\n",
    "from pykalman import KalmanFilter\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym import spaces\n",
    "from scipy.signal import periodogram\n",
    "from scipy.stats import norm\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self, data, window_size=10, initial_balance=10000, n_envs=1):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "\n",
    "        # Initialize instance variables\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.initial_balance = initial_balance\n",
    "        self.positions = {}\n",
    "        self.current_step = self.window_size\n",
    "        self.done = False\n",
    "\n",
    "        # Extract tickers from data\n",
    "        self.tickers = list(set(data['Ticker']))\n",
    "\n",
    "        # Set action and observation spaces\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(len(self.tickers),), dtype=np.float32)\n",
    "        #TODO derive the 33451 number from the variables at play in next_observation\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1, 33451), dtype=np.float32)\n",
    "\n",
    "        # Set number of environments to 1\n",
    "        self.n_envs = n_envs\n",
    "\n",
    "        # Set up the observation buffer\n",
    "        self._obs_buffer = np.zeros((self.n_envs,) + self.observation_space.shape, dtype=self.observation_space.dtype)\n",
    "\n",
    "        # Reset the environment\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.balance = self.initial_balance\n",
    "        self.current_step = self.window_size\n",
    "        self.done = False\n",
    "        self.positions = {}\n",
    "\n",
    "        obs = np.empty((self.n_envs,) + self.observation_space.shape, dtype=self.observation_space.dtype)\n",
    "        for i in range(self.n_envs):\n",
    "            obs[i] = self._next_observation()\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        current_prices = self.data.iloc[self.current_step][['Close', 'Ticker']].values\n",
    "        current_prices = np.expand_dims(current_prices, axis=0)\n",
    "        current_prices = np.tile(current_prices, (len(self.tickers), 1))\n",
    "        actions = action * self.balance\n",
    "\n",
    "        for i, t in enumerate(self.tickers):\n",
    "            if actions[i] > 0:\n",
    "                shares_to_buy = int(actions[i] // current_prices[i, 0])\n",
    "                cost = shares_to_buy * current_prices[i, 0]\n",
    "                if cost > self.balance:\n",
    "                    shares_to_buy = 0\n",
    "                    cost = 0\n",
    "                else:\n",
    "                    self.balance -= cost\n",
    "                if t not in self.positions:\n",
    "                    self.positions[t] = 0\n",
    "                self.positions[t] += shares_to_buy\n",
    "            elif actions[i] < 0:\n",
    "                shares_to_sell = int(min(self.positions.get(t, 0), np.abs(actions[i]) // current_prices[i, 0]))\n",
    "                if shares_to_sell > 0:\n",
    "                    self.positions[t] -= shares_to_sell\n",
    "                    self.balance += shares_to_sell * current_prices[i, 0]\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            self.done = True\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        reward = self.balance - self.initial_balance\n",
    "        done = self.done\n",
    "        info = {}\n",
    "\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def _next_observation(self):\n",
    "        price_data = []\n",
    "        for t in self.tickers:\n",
    "            data_t = self.data[self.data['Ticker'] == t].iloc[self.current_step - self.window_size:self.current_step]\n",
    "            if len(data_t) < self.window_size:\n",
    "                data_t = pd.concat([self.data[self.data['Ticker'] == t].iloc[:self.current_step], data_t], axis=0)\n",
    "            price_data_t = data_t[['Close']].values\n",
    "            price_data.append(price_data_t)\n",
    "\n",
    "        # Reshape normalized price data into flattened observation vector\n",
    "        obs = np.hstack(price_data).flatten()\n",
    "\n",
    "        # Replace any NaN values with zeros\n",
    "        obs = np.nan_to_num(obs)\n",
    "\n",
    "        # Estimate the underlying asset price dynamics using the Kalman filter for each ticker\n",
    "        smoothed_prices = np.zeros((self.window_size, 5*len(self.tickers)))\n",
    "        for i, t in enumerate(self.tickers):\n",
    "            price_data_t = price_data[i]\n",
    "            kf = KalmanFilter(initial_state_mean=price_data_t[0], n_dim_obs=1)\n",
    "            smoothed_prices_t, _ = kf.smooth(price_data_t.flatten())\n",
    "            smoothed_prices[:,5*i:5*(i+1)] = smoothed_prices_t.reshape(self.window_size, 1)\n",
    "\n",
    "        # Isolate cyclic components using Fourier-based spectral estimation for each ticker\n",
    "        cyclic_components = np.zeros(self.window_size)\n",
    "        for i, t in enumerate(self.tickers):\n",
    "            price_data_t = price_data[i]\n",
    "            freqs, psd = periodogram(price_data_t)\n",
    "            significant_freqs = freqs[np.argsort(psd)[:5]]  # Top 5 frequencies with lowest power spectral density\n",
    "            cyclic_components_t = np.sum([np.sin(2 * np.pi * f * np.arange(len(price_data_t))) for f in significant_freqs], axis=0)\n",
    "            cyclic_components += cyclic_components_t\n",
    "\n",
    "        # Model the underlying asset price dynamics using Geometric Brownian Motion (GBM) for each ticker\n",
    "        price_dynamics = np.zeros((self.window_size, len(self.tickers)))\n",
    "        for i, t in enumerate(self.tickers):\n",
    "            price_data_t = price_data[i]\n",
    "            returns = np.diff(price_data_t, axis=0) / price_data_t[:-1]\n",
    "            mu = np.mean(returns, axis=0)\n",
    "            sigma = np.std(returns, axis=0)\n",
    "            dt = 1\n",
    "            Z = norm.ppf(np.random.rand(self.window_size - 1))\n",
    "            price_dynamics_t = price_data_t[0] * np.exp(np.cumsum((mu - 0.5 * sigma**2) * dt + sigma * np.sqrt(dt) * Z))\n",
    "            price_dynamics[:len(price_dynamics_t),i] = price_dynamics_t\n",
    "\n",
    "        # Create portfolio feature array\n",
    "        portfolio_features = np.zeros((1, len(self.tickers)))\n",
    "        for i, t in enumerate(self.tickers):\n",
    "            portfolio_features[0,i] = self.positions.get(t, 0)\n",
    "\n",
    "        # Combine all features into a single observation array\n",
    "        obs = np.concatenate([obs, smoothed_prices.flatten(), cyclic_components, price_dynamics.flatten(), portfolio_features.flatten()])\n",
    "\n",
    "        return obs.reshape(1, -1)\n",
    "\n",
    "# Define the optimization function\n",
    "def optimize_agent(trial):\n",
    "    # Define hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.99, 0.999])\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 2, 10)\n",
    "    n_steps = trial.suggest_int(\"n_steps\", 16, 2048)\n",
    "\n",
    "    # Train the agent using PPO algorithm\n",
    "    model = PPO(\"MlpPolicy\", env, learning_rate=learning_rate, gamma=gamma, n_epochs=n_epochs, n_steps=n_steps, verbose=0)\n",
    "    model.learn(total_timesteps=100000)\n",
    "\n",
    "    return evaluate_agent(model)\n",
    "\n",
    "# Evaluate the agent\n",
    "def evaluate_agent(model):\n",
    "    # Test the agent's performance\n",
    "    test_env = gym.make(\"stocks-v0\", df=ohlcv_data)\n",
    "    test_env = DummyVecEnv([lambda: test_env])\n",
    "\n",
    "    obs = test_env.reset()\n",
    "    done = False\n",
    "    total_profits = []\n",
    "\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, info = test_env.step(action)\n",
    "        total_profits.append(info[0]['total_profit'])\n",
    "\n",
    "    # Calculate portfolio returns\n",
    "    portfolio_returns = np.diff(total_profits) / total_profits[:-1]\n",
    "    annualized_return = np.mean(portfolio_returns) * 252\n",
    "\n",
    "    #TODO change to use the actual snp500 pct change\n",
    "    # Calculate benchmark returns (S&P 500 Index)\n",
    "    benchmark_returns = ohlcv_data['Close'].pct_change().dropna().values\n",
    "\n",
    "    return annualized_return, calculate_performance_metrics(portfolio_returns, benchmark_returns)\n",
    "\n",
    "def calculate_performance_metrics(portfolio_returns, benchmark_returns, risk_free_rate=0.02, sortino_target=0.0):\n",
    "    # Annualize returns\n",
    "    annualized_return = np.mean(portfolio_returns) * 252\n",
    "    benchmark_annualized_return = np.mean(benchmark_returns) * 252\n",
    "\n",
    "    # Max drawdown\n",
    "    cum_returns = (1 + portfolio_returns).cumprod()\n",
    "    running_max = np.maximum.accumulate(cum_returns)\n",
    "    drawdown = (cum_returns / running_max) - 1\n",
    "    max_drawdown = np.min(drawdown)\n",
    "\n",
    "    # Sharpe ratio\n",
    "    sharpe_ratio = (annualized_return - risk_free_rate) / (np.std(portfolio_returns) * np.sqrt(252))\n",
    "    benchmark_sharpe_ratio = (benchmark_annualized_return - risk_free_rate) / (np.std(benchmark_returns) * np.sqrt(252))\n",
    "\n",
    "    # Sortino ratio\n",
    "    downside_returns = portfolio_returns.copy()\n",
    "    downside_returns[downside_returns > sortino_target] = 0\n",
    "    sortino_ratio = (annualized_return - risk_free_rate) / (np.std(downside_returns) * np.sqrt(252))\n",
    "\n",
    "    # Performance relative to the S&P 500 Index on a risk-adjusted basis\n",
    "    risk_adjusted_performance = sharpe_ratio / benchmark_sharpe_ratio\n",
    "\n",
    "    return {\n",
    "        'annualized_return': annualized_return,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'benchmark_sharpe_ratio': benchmark_sharpe_ratio,\n",
    "        'sortino_ratio': sortino_ratio,\n",
    "        'risk_adjusted_performance': risk_adjusted_performance\n",
    "    }\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T17:45:02.777722Z",
     "end_time": "2023-04-25T17:45:02.779003Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "OPTIMIZATION_ENABLED = False  # Set this to False to disable the optimization process\n",
    "\n",
    "# Load data for the environment\n",
    "ohlcv_data = pd.read_csv(\"sp500_ohlcv_data.csv\", index_col=0, parse_dates=True)\n",
    "\n",
    "# Initialize the environment\n",
    "env = DummyVecEnv([lambda: StockTradingEnv(ohlcv_data) for _ in range(1)])\n",
    "\n",
    "# Run the optimization if enabled\n",
    "if OPTIMIZATION_ENABLED:\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(optimize_agent, n_trials=100)\n",
    "    best_params = study.best_params\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "else:\n",
    "    best_params = {}\n",
    "\n",
    "# Train the PPO agent with the best hyperparameters\n",
    "best_model = PPO(\"MlpPolicy\", env, **best_params, verbose=1)\n",
    "best_model.learn(total_timesteps=1000)\n",
    "\n",
    "# Calculate and print performance metrics\n",
    "annualized_return, metrics = evaluate_agent(best_model)\n",
    "print(\"Annualized return:\", annualized_return)\n",
    "print(metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T16:27:07.975465Z",
     "end_time": "2023-04-25T16:27:12.691206Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
